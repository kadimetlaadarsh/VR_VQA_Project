{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11782776,"sourceType":"datasetVersion","datasetId":7397658},{"sourceId":11843558,"sourceType":"datasetVersion","datasetId":7441274}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:09.647243Z","iopub.execute_input":"2025-05-18T04:00:09.648012Z","iopub.status.idle":"2025-05-18T04:00:09.651858Z","shell.execute_reply.started":"2025-05-18T04:00:09.647988Z","shell.execute_reply":"2025-05-18T04:00:09.651082Z"}},"outputs":[],"execution_count":466},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom PIL import Image, UnidentifiedImageError\nfrom transformers import ViltProcessor, ViltForQuestionAnswering, TrainingArguments, Trainer, ViltConfig, logging\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom peft import get_peft_model, LoraConfig\nfrom sklearn.metrics import classification_report\nimport time\nimport transformers\n\n# Print transformers version for debugging\nprint(f\"Transformers version: {transformers.__version__}\")\n\n# Configure transformers logger to show INFO level\nlogging.set_verbosity_warning()\nlogger = logging.get_logger(\"transformers\")\nlogger.info(\"Logging configured to INFO level\")\n# === Custom Callback to Print Loss ===\n# class PrintLossCallback(transformers.TrainerCallback):\n#     def on_log(self, args, state, control, logs=None, **kwargs):\n#         #if logs and \"loss\" in logs:\n#             print(f\"Step {state.global_step}/{state.max_steps}, Training Loss: {logs['loss']:.4f}\")\n\n# === Custom ViLT model for single-label classification ===\nclass CustomViltForQuestionAnswering(ViltForQuestionAnswering):\n    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None, **kwargs):\n        outputs = super().forward(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        logits = outputs.logits  # Shape: [batch_size, num_classes]\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits, labels)\n\n        return transformers.modeling_outputs.SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:09.653093Z","iopub.execute_input":"2025-05-18T04:00:09.653811Z","iopub.status.idle":"2025-05-18T04:00:09.670178Z","shell.execute_reply.started":"2025-05-18T04:00:09.653788Z","shell.execute_reply":"2025-05-18T04:00:09.669660Z"}},"outputs":[{"name":"stdout","text":"Transformers version: 4.51.3\n","output_type":"stream"}],"execution_count":467},{"cell_type":"code","source":"# === Custom Trainer to Handle Empty Evaluation Metrics ===\nclass CustomTrainer(Trainer):\n    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate):\n        super()._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\n        if self.control.should_evaluate and not self.state.eval_metrics:\n            print(f\"Warning: No evaluation metrics produced at epoch {epoch}. Skipping best model check.\")\n            self.state.eval_metrics = {\"eval_loss\": float(\"inf\")}  # Fallback to avoid KeyError\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:09.670830Z","iopub.execute_input":"2025-05-18T04:00:09.671011Z","iopub.status.idle":"2025-05-18T04:00:09.684129Z","shell.execute_reply.started":"2025-05-18T04:00:09.670997Z","shell.execute_reply":"2025-05-18T04:00:09.683491Z"}},"outputs":[],"execution_count":468},{"cell_type":"code","source":"# === Load CSV dataset ===\ndf = pd.read_csv(\"/kaggle/input/merged-vqa-dataset/merged_vqa_dataset_output.csv\")\nBASE_IMAGE_DIR = \"/kaggle/input/abo-dataset/images/small\"\n\n# === Load ViLT config for answer list ===\nconfig = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\noriginal_answers = list(config.id2label.values())\nvalid_answer_set = set(original_answers)\nanswer2id = {ans: idx for idx, ans in enumerate(original_answers)}\nnum_classes = len(answer2id)\nprint(f\"Number of ViLT answer classes: {num_classes}\")\nprint(f\"Sample valid answers: {list(valid_answer_set)[:10]}\")\n\n# === Flatten dataset ===\nsamples = []\nunmatched_answers = set()\n\nfor _, row in df.iterrows():\n    for q_col, a_col in zip(['q1', 'q2', 'q3'], ['a1', 'a2', 'a3']):\n        answer = str(row[a_col]).strip().lower()\n        if pd.notna(answer) and answer in valid_answer_set:\n            samples.append({\n                \"image_path\": os.path.join(BASE_IMAGE_DIR, row['path']),\n                \"question\": row[q_col],\n                \"answer\": answer\n            })\n        else:\n            unmatched_answers.add(answer)\n\nprint(f\"Filtered samples count: {len(samples)}\")\nprint(f\"Unmatched answers: {len(unmatched_answers)}\")\nprint(f\"Example unmatched answers: {list(unmatched_answers)[:10]}\")\n\n# === Split dataset ===\ntrain_df, test_df = train_test_split(pd.DataFrame(samples), test_size=0.2, random_state=42)\n\n# === Check answer distribution ===\nprint(\"Training answer distribution:\\n\", train_df['answer'].value_counts())\nprint(\"Test answer distribution:\\n\", test_df['answer'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:09.685303Z","iopub.execute_input":"2025-05-18T04:00:09.685611Z","iopub.status.idle":"2025-05-18T04:00:10.643837Z","shell.execute_reply.started":"2025-05-18T04:00:09.685590Z","shell.execute_reply":"2025-05-18T04:00:10.643150Z"}},"outputs":[{"name":"stdout","text":"Number of ViLT answer classes: 3129\nSample valid answers: ['4:30', 'cabbage', 'younger', 'towards', 'on motorcycle', 'dinosaur', '3', 'parakeet', 'paint', 'texting']\nFiltered samples count: 33716\nUnmatched answers: 2187\nExample unmatched answers: ['13.7in', 'hangers', 'thirty', '382mm', '2000mm', 'kneepads', 'fryer', 'mailer', 'nozzle', '5mm']\nTraining answer distribution:\n answer\nblack       2601\ncase        2358\nwhite       2285\nblue        1109\nbrown        891\n            ... \nchili          1\nspoons         1\n75             1\ndeer           1\nclimbing       1\nName: count, Length: 796, dtype: int64\nTest answer distribution:\n answer\nblack        716\ncase         550\nwhite        500\nblue         259\nbrown        221\n            ... \nmixer          1\nfeet           1\ncheckered      1\nspinach        1\nporcelain      1\nName: count, Length: 524, dtype: int64\n","output_type":"stream"}],"execution_count":469},{"cell_type":"code","source":"# === Check answer distribution ===\nprint(\"Training answer distribution:\\n\", train_df['answer'].value_counts())\nprint(\"Test answer distribution:\\n\", test_df['answer'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:10.644402Z","iopub.execute_input":"2025-05-18T04:00:10.644595Z","iopub.status.idle":"2025-05-18T04:00:10.654783Z","shell.execute_reply.started":"2025-05-18T04:00:10.644580Z","shell.execute_reply":"2025-05-18T04:00:10.654052Z"}},"outputs":[{"name":"stdout","text":"Training answer distribution:\n answer\nblack       2601\ncase        2358\nwhite       2285\nblue        1109\nbrown        891\n            ... \nchili          1\nspoons         1\n75             1\ndeer           1\nclimbing       1\nName: count, Length: 796, dtype: int64\nTest answer distribution:\n answer\nblack        716\ncase         550\nwhite        500\nblue         259\nbrown        221\n            ... \nmixer          1\nfeet           1\ncheckered      1\nspinach        1\nporcelain      1\nName: count, Length: 524, dtype: int64\n","output_type":"stream"}],"execution_count":470},{"cell_type":"code","source":"# === Validate and Filter dataset ===\ndef validate_and_filter_dataset(df, base_dir):\n    invalid_images = []\n    invalid_labels = []\n    valid_rows = []\n    for idx, row in df.iterrows():\n        image_path = os.path.join(base_dir, row['image_path'])\n        answer = row['answer']\n        if not os.path.exists(image_path):\n            invalid_images.append(image_path)\n        elif answer not in valid_answer_set:\n            invalid_labels.append(answer)\n        else:\n            valid_rows.append(row)\n    print(f\"Invalid images: {len(invalid_images)}\")\n    if invalid_images:\n        print(f\"Sample invalid images: {invalid_images[:5]}\")\n    print(f\"Invalid labels: {len(invalid_labels)}\")\n    if invalid_labels:\n        print(f\"Sample invalid labels: {invalid_labels[:5]}\")\n    print(f\"Valid samples after filtering: {len(valid_rows)}\")\n    return pd.DataFrame(valid_rows)\n\nprint(\"\\nValidating and filtering training dataset:\")\ntrain_df = validate_and_filter_dataset(train_df, BASE_IMAGE_DIR)\nprint(\"\\nValidating and filtering test dataset:\")\ntest_df = validate_and_filter_dataset(test_df, BASE_IMAGE_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:10.655586Z","iopub.execute_input":"2025-05-18T04:00:10.655858Z","iopub.status.idle":"2025-05-18T04:00:21.420493Z","shell.execute_reply.started":"2025-05-18T04:00:10.655838Z","shell.execute_reply":"2025-05-18T04:00:21.419898Z"}},"outputs":[{"name":"stdout","text":"\nValidating and filtering training dataset:\nInvalid images: 0\nInvalid labels: 0\nValid samples after filtering: 26972\n\nValidating and filtering test dataset:\nInvalid images: 0\nInvalid labels: 0\nValid samples after filtering: 6744\n","output_type":"stream"}],"execution_count":471},{"cell_type":"code","source":"# === Load ViLT processor ===\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.422740Z","iopub.execute_input":"2025-05-18T04:00:21.422962Z","iopub.status.idle":"2025-05-18T04:00:21.756841Z","shell.execute_reply.started":"2025-05-18T04:00:21.422944Z","shell.execute_reply":"2025-05-18T04:00:21.756325Z"}},"outputs":[],"execution_count":472},{"cell_type":"code","source":"# # === Create answer2id mapping based on your dataset's unique answers ===\n# all_answers = pd.concat([train_df['answer'], test_df['answer']]).unique()\n# answer2id = {ans: idx for idx, ans in enumerate(sorted(all_answers))}\n# num_classes = len(answer2id)\n# print(f\"Number of unique answers (classes): {num_classes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.757476Z","iopub.execute_input":"2025-05-18T04:00:21.757677Z","iopub.status.idle":"2025-05-18T04:00:21.761086Z","shell.execute_reply.started":"2025-05-18T04:00:21.757662Z","shell.execute_reply":"2025-05-18T04:00:21.760398Z"}},"outputs":[],"execution_count":473},{"cell_type":"code","source":"# # Use ViLT config's id2label mapping directly\n# config = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n# original_answers = list(config.id2label.values())  # id2label is a dict: {int: str}\n# answer2id = {ans: idx for idx, ans in enumerate(original_answers)}  # invert it\n# num_classes = len(answer2id)\n# print(f\"Number of fixed ViLT answer classes: {num_classes}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.761864Z","iopub.execute_input":"2025-05-18T04:00:21.762058Z","iopub.status.idle":"2025-05-18T04:00:21.771104Z","shell.execute_reply.started":"2025-05-18T04:00:21.762040Z","shell.execute_reply":"2025-05-18T04:00:21.770441Z"}},"outputs":[],"execution_count":474},{"cell_type":"code","source":"train_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Remove the index column added by from_pandas\ntrain_dataset = train_dataset.remove_columns(['__index_level_0__'])\ntest_dataset = test_dataset.remove_columns(['__index_level_0__'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.771736Z","iopub.execute_input":"2025-05-18T04:00:21.771919Z","iopub.status.idle":"2025-05-18T04:00:21.815971Z","shell.execute_reply.started":"2025-05-18T04:00:21.771900Z","shell.execute_reply":"2025-05-18T04:00:21.815446Z"}},"outputs":[],"execution_count":475},{"cell_type":"code","source":"# === Remove unnecessary columns ===\nwanted_cols = {\"image_path\", \"question\", \"answer\"}\nfor ds in [train_dataset, test_dataset]:\n    extra_cols = [col for col in ds.column_names if col not in wanted_cols]\n    if extra_cols:\n        ds = ds.remove_columns(extra_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.816675Z","iopub.execute_input":"2025-05-18T04:00:21.816868Z","iopub.status.idle":"2025-05-18T04:00:21.821053Z","shell.execute_reply.started":"2025-05-18T04:00:21.816853Z","shell.execute_reply":"2025-05-18T04:00:21.820388Z"}},"outputs":[],"execution_count":476},{"cell_type":"code","source":"# === Print dataset sizes ===\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.821679Z","iopub.execute_input":"2025-05-18T04:00:21.821969Z","iopub.status.idle":"2025-05-18T04:00:21.833788Z","shell.execute_reply.started":"2025-05-18T04:00:21.821892Z","shell.execute_reply":"2025-05-18T04:00:21.833101Z"}},"outputs":[{"name":"stdout","text":"Training dataset size: 26972\nTest dataset size: 6744\n","output_type":"stream"}],"execution_count":477},{"cell_type":"code","source":"# # === Initialize ViLT processor ===\n# processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.834697Z","iopub.execute_input":"2025-05-18T04:00:21.834978Z","iopub.status.idle":"2025-05-18T04:00:21.843380Z","shell.execute_reply.started":"2025-05-18T04:00:21.834957Z","shell.execute_reply":"2025-05-18T04:00:21.842791Z"}},"outputs":[],"execution_count":478},{"cell_type":"code","source":"from torchvision import transforms\n\nIMAGE_SIZE = (384, 384)\n\n# Update safe_open_image to resize images\ndef safe_open_image(path):\n    try:\n        image = Image.open(path).convert(\"RGB\")\n        image = image.resize(IMAGE_SIZE)  # Resize here\n        return image\n    except Exception as e:\n        print(f\"Error opening image {path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.844130Z","iopub.execute_input":"2025-05-18T04:00:21.844425Z","iopub.status.idle":"2025-05-18T04:00:21.853595Z","shell.execute_reply.started":"2025-05-18T04:00:21.844401Z","shell.execute_reply":"2025-05-18T04:00:21.852858Z"}},"outputs":[],"execution_count":479},{"cell_type":"code","source":"# === Preprocessing / Transform function ===\ndef transform(batch, is_eval=False):\n    try:\n        images = [safe_open_image(p) for p in batch[\"image_path\"]]\n        valid_data = [(img, q, a) for img, q, a in zip(images, batch[\"question\"], batch[\"answer\"]) if img is not None]\n        if not valid_data:\n            print(f\"Warning: All images in batch are invalid ({'eval' if is_eval else 'train'}), returning None\")\n            return None  # Skip empty batches\n\n        imgs, questions, answers = zip(*valid_data)\n\n        encoding = processor(\n            images=imgs,\n            text=questions,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=32,\n            return_tensors=\"pt\",\n        )\n\n        label_indices = [answer2id.get(ans, -1) for ans in answers]\n        valid_indices = [i for i, idx in enumerate(label_indices) if idx != -1]\n        if not valid_indices:\n            print(f\"Warning: No valid labels in batch ({'eval' if is_eval else 'train'}), returning None\")\n            return None  # Skip empty batches\n        \n        encoding = {k: v[valid_indices] for k, v in encoding.items()}\n        labels = torch.tensor([label_indices[i] for i in valid_indices], dtype=torch.long)\n\n        batch = {\n            \"pixel_values\": encoding[\"pixel_values\"],\n            \"input_ids\": encoding[\"input_ids\"],\n            \"attention_mask\": encoding[\"attention_mask\"],\n            \"labels\": labels\n        }\n\n        #if is_eval:\n            #print(f\"Valid evaluation batch processed, size: {len(labels)}\")\n        return batch\n    except Exception as e:\n        print(f\"Error in transform ({'eval' if is_eval else 'train'}): {e}\")\n        return None  # Skip problematic batches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.856207Z","iopub.execute_input":"2025-05-18T04:00:21.856419Z","iopub.status.idle":"2025-05-18T04:00:21.865588Z","shell.execute_reply.started":"2025-05-18T04:00:21.856405Z","shell.execute_reply":"2025-05-18T04:00:21.864773Z"}},"outputs":[],"execution_count":480},{"cell_type":"code","source":"# # Apply the transform function to datasets\n# train_dataset = train_dataset.map(transform, batched=True, batch_size=4, remove_columns=train_dataset.column_names)\n# test_dataset = test_dataset.map(transform, batched=True, batch_size=4, remove_columns=test_dataset.column_names)\n# === Apply transform ===\ntrain_dataset = train_dataset.with_transform(lambda batch: transform(batch, is_eval=False))\ntest_dataset = test_dataset.with_transform(lambda batch: transform(batch, is_eval=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.866390Z","iopub.execute_input":"2025-05-18T04:00:21.866667Z","iopub.status.idle":"2025-05-18T04:00:21.923682Z","shell.execute_reply.started":"2025-05-18T04:00:21.866643Z","shell.execute_reply":"2025-05-18T04:00:21.923212Z"}},"outputs":[],"execution_count":481},{"cell_type":"code","source":"# === Load ViLT model ===\nmodel = CustomViltForQuestionAnswering.from_pretrained(\n    \"dandelin/vilt-b32-finetuned-vqa\",\n    num_labels=num_classes,\n    ignore_mismatched_sizes=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:21.924237Z","iopub.execute_input":"2025-05-18T04:00:21.924390Z","iopub.status.idle":"2025-05-18T04:00:22.451910Z","shell.execute_reply.started":"2025-05-18T04:00:21.924378Z","shell.execute_reply":"2025-05-18T04:00:22.451375Z"}},"outputs":[],"execution_count":482},{"cell_type":"code","source":"# === Apply LoRA ===\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\",\n    target_modules=[\"query\", \"key\", \"value\"],\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n# === Disable DataParallel for single GPU ===\n# if torch.cuda.device_count() <= 1:\n#     print(\"Using single GPU, disabling DataParallel\")\n#     model = model\n# else:\n#     print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n#     model = torch.nn.DataParallel(model)\n\n# === Training arguments ===\ntraining_args = TrainingArguments(\n    output_dir=\"./vilt-lora-vqa\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    num_train_epochs=5,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=50,\n    logging_strategy=\"steps\",\n    remove_unused_columns=False,\n    report_to=\"none\",\n)\n# === Calculate expected steps ===\nsteps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\ntotal_steps = steps_per_epoch * training_args.num_train_epochs\nprint(f\"Expected steps per epoch: {steps_per_epoch}\")\nprint(f\"Total expected steps: {total_steps}\")\n\n# === Initialize Trainer ===\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    #callbacks=[PrintLossCallback()],\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:22.452729Z","iopub.execute_input":"2025-05-18T04:00:22.452998Z","iopub.status.idle":"2025-05-18T04:00:22.712224Z","shell.execute_reply.started":"2025-05-18T04:00:22.452966Z","shell.execute_reply":"2025-05-18T04:00:22.711456Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 884,736 || all params: 118,473,273 || trainable%: 0.7468\nExpected steps per epoch: 3371\nTotal expected steps: 16855\n","output_type":"stream"}],"execution_count":483},{"cell_type":"code","source":"# === Train ===\ntry:\n    train_results = trainer.train()\n    trainer.save_model(\"./vilt-lora-vqa-final\")\nexcept Exception as e:\n    print(f\"Training failed: {e}\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:00:22.713079Z","iopub.execute_input":"2025-05-18T04:00:22.713548Z","iopub.status.idle":"2025-05-18T04:58:49.349711Z","shell.execute_reply.started":"2025-05-18T04:00:22.713509Z","shell.execute_reply":"2025-05-18T04:58:49.348977Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8430' max='8430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8430/8430 58:25, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>7.707300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.829000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>5.436100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>4.736500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>5.001500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>4.506700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>4.538500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>4.725200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>4.815900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>4.355300</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>4.782400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>4.632300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>4.067600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>4.643300</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>4.290900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>4.509200</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>4.142000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>4.156700</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>3.630500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>4.109700</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>3.876100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>4.465300</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>4.079900</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.998900</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>4.403500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.879300</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>3.971800</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.911200</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>3.809100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.635700</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>3.884500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>4.091600</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>3.508400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>3.412000</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>3.487200</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>3.727700</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>3.162000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>3.306400</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>3.454500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.611700</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>3.449000</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>3.705200</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>3.431300</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>3.971700</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>3.821500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>3.004400</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>3.155700</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>3.333100</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>3.402400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>3.289700</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>3.627200</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>3.277000</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>3.856400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>3.087500</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>3.721000</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>3.756100</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>3.372500</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>3.263300</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>3.477900</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>3.421000</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>3.049300</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>3.405400</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>3.535500</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>3.602400</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>3.598400</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>3.583900</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>3.289400</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>3.106600</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>2.933200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>3.180400</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>2.948900</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>3.018500</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>3.066500</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>3.419000</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>3.162200</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>3.645300</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>3.000600</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>2.943700</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>3.439200</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>3.004400</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>3.293800</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>3.284800</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>2.455200</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>3.160500</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>2.666600</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>3.033900</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>2.925600</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>3.382600</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>3.361800</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>3.214500</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>3.321900</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>2.765200</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>2.788200</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>2.932900</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>2.829100</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>2.750700</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>3.080500</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>3.202300</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>3.274400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.736500</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>3.100600</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>2.689600</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>2.879200</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>2.862400</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>2.758500</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>3.064100</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>2.474900</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>2.970700</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>2.810700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>2.812900</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>2.902500</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>2.750900</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>3.337100</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>2.929700</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>2.779500</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>2.870600</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>2.790300</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>3.067600</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>2.579600</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>3.179400</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>2.957600</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>2.565800</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>2.732900</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>2.839000</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>2.582700</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>2.639000</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>2.632200</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>3.093300</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>2.351500</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>2.683900</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>2.497200</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>2.959800</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>3.109000</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>2.585900</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>2.889600</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>2.801900</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>2.499900</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>2.701200</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>3.114200</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>2.357200</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>2.921700</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>2.745800</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>2.781900</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>2.807100</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>2.894700</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>2.704400</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>2.998900</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>2.755000</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>2.582000</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>2.380100</td>\n    </tr>\n    <tr>\n      <td>7550</td>\n      <td>2.811400</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>2.802600</td>\n    </tr>\n    <tr>\n      <td>7650</td>\n      <td>2.370300</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>2.256300</td>\n    </tr>\n    <tr>\n      <td>7750</td>\n      <td>2.577400</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>2.746600</td>\n    </tr>\n    <tr>\n      <td>7850</td>\n      <td>2.657100</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>2.648300</td>\n    </tr>\n    <tr>\n      <td>7950</td>\n      <td>2.447900</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>2.349600</td>\n    </tr>\n    <tr>\n      <td>8050</td>\n      <td>2.720200</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>2.562300</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>2.344800</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>2.649100</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>2.806400</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>2.880400</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>2.453900</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>2.748000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":484},{"cell_type":"code","source":"# all_answers = pd.concat([train_df['answer'], test_df['answer']]).str.lower().unique()\n# answer2id = {ans: idx for idx, ans in enumerate(sorted(all_answers))}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.350420Z","iopub.execute_input":"2025-05-18T04:58:49.350681Z","iopub.status.idle":"2025-05-18T04:58:49.354287Z","shell.execute_reply.started":"2025-05-18T04:58:49.350664Z","shell.execute_reply":"2025-05-18T04:58:49.353512Z"}},"outputs":[],"execution_count":485},{"cell_type":"code","source":"# # Example: build label mapping from your dataset\n# unique_answers = list(set(example['answer'] for example in train_dataset))\n# answer2label = {ans: i for i, ans in enumerate(unique_answers)}\n# label2answer = {i: ans for ans, i in answer2label.items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.355091Z","iopub.execute_input":"2025-05-18T04:58:49.355284Z","iopub.status.idle":"2025-05-18T04:58:49.365703Z","shell.execute_reply.started":"2025-05-18T04:58:49.355269Z","shell.execute_reply":"2025-05-18T04:58:49.365083Z"}},"outputs":[],"execution_count":486},{"cell_type":"code","source":"# wanted_cols = {\"image_path\", \"question\", \"answer\"}\n# for ds in [train_dataset, test_dataset]:\n#     extra_cols = [col for col in ds.column_names if col not in wanted_cols]\n#     if extra_cols:\n#         ds = ds.remove_columns(extra_cols)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.366392Z","iopub.execute_input":"2025-05-18T04:58:49.366980Z","iopub.status.idle":"2025-05-18T04:58:49.377371Z","shell.execute_reply.started":"2025-05-18T04:58:49.366957Z","shell.execute_reply":"2025-05-18T04:58:49.376862Z"}},"outputs":[],"execution_count":487},{"cell_type":"code","source":"# # === Initialize processor ===\n# processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n\n# # === Preprocessing function ===\n# # def transform(batch):\n# #     image = Image.open(batch[\"image_path\"]).convert(\"RGB\")\n# #     encoding = processor(\n# #         image, batch[\"question\"], padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\"\n# #     )\n# #     labels = processor.tokenizer(\n# #         batch[\"answer\"], padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\"\n# #     ).input_ids[0]\n    \n# #     batch[\"input_ids\"] = encoding.input_ids[0]\n# #     batch[\"attention_mask\"] = encoding.attention_mask[0]\n# #     batch[\"pixel_values\"] = encoding.pixel_values[0]\n# #     batch[\"labels\"] = labels\n# #     return batch\n# def transform(batch):\n    \n#     images = [safe_open_image(p) for p in batch[\"image_path\"]]\n#     valid_data = [(img, q, a) for img, q, a in zip(images, batch[\"question\"], batch[\"answer\"]) if img is not None]\n\n#     if not valid_data:\n#         raise ValueError(\"All images in the batch are invalid!\")\n\n#     imgs, questions, answers = zip(*valid_data)\n\n#     encoding = processor(\n#         images,\n#         batch[\"question\"],\n#         return_tensors=\"pt\",\n#         padding=\"max_length\",\n#         truncation=True,\n#         max_length=32\n#     )\n#     # labels = processor.tokenizer(\n#     #     batch[\"answer\"],\n#     #     padding=\"max_length\",\n#     #     truncation=True,\n#     #     max_length=32,\n#     #     return_tensors=\"pt\"\n#     # ).input_ids\n#         # Convert answer to index (make sure answer2id dict is defined)\n#     label_indices = [answer2id[ans.lower()] for ans in batch[\"answer\"]]\n\n#     # One-hot encode\n#     labels = torch.zeros((len(label_indices), len(answer2id)), dtype=torch.float32)\n#     for i, idx in enumerate(label_indices):\n#         labels[i, idx] = 1.0\n\n#     batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n#     batch[\"input_ids\"] = encoding[\"input_ids\"]\n#     batch[\"attention_mask\"] = encoding[\"attention_mask\"]\n#     #batch[\"labels\"] = torch.tensor([answer2id[a] for a in batch[\"answer\"]], dtype=torch.float32)\n#     batch[\"labels\"] = labels\n#     return batch\n\n\n# # # train_dataset = train_dataset.map(transform)\n# # # test_dataset = test_dataset.map(transform)\n# # def make_transform(tokenizer, processor, answer2label):\n# #     def transform(example):\n# #         question = example[\"question\"]\n# #         image_path = example[\"image_path\"]\n# #         answer = example[\"answer\"]\n\n# #         # Load and process image\n# #         image = Image.open(image_path).convert(\"RGB\")\n# #         encoding = processor(\n# #         images=image,\n# #         text=question,\n# #         return_tensors=\"pt\",\n# #         padding=\"max_length\",\n# #         truncation=True,\n# #         max_length=40   # This is the fix!\n# #         )\n# #         # Flatten tensor dimensions\n# #         encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n\n# #         # Label as index\n# #         encoding[\"labels\"] = torch.tensor(answer2label.get(answer, -100), dtype=torch.long)\n# #         return encoding\n\n# #     return transform\n# train_dataset.set_transform(transform)\n# test_dataset.set_transform(transform)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.378242Z","iopub.execute_input":"2025-05-18T04:58:49.378422Z","iopub.status.idle":"2025-05-18T04:58:49.392186Z","shell.execute_reply.started":"2025-05-18T04:58:49.378409Z","shell.execute_reply":"2025-05-18T04:58:49.391649Z"}},"outputs":[],"execution_count":488},{"cell_type":"code","source":"# print(train_dataset.features)\n# #Remove unwanted columns before applying the transformation\n# train_dataset = train_dataset.remove_columns(['__index_level_0__'])\n# test_dataset = test_dataset.remove_columns(['__index_level_0__'])\n# print(train_dataset.features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.392815Z","iopub.execute_input":"2025-05-18T04:58:49.393035Z","iopub.status.idle":"2025-05-18T04:58:49.404852Z","shell.execute_reply.started":"2025-05-18T04:58:49.393013Z","shell.execute_reply":"2025-05-18T04:58:49.404328Z"}},"outputs":[],"execution_count":489},{"cell_type":"code","source":"# import torch.nn as nn  # Import the nn module\n# # === Load model and apply LoRA ===\n# model = ViltForQuestionAnswering.from_pretrained(\n#     \"dandelin/vilt-b32-finetuned-vqa\",\n#     num_labels=len(answer2id),\n#     ignore_mismatched_sizes=True  # <-- this is key\n# )\n# # # Adjust the classification head to output 32 logits (for 32 possible answers)\n# # num_classes = len(unique_answers)  # This is 3442 based on your dataset\n# # model.classifier = nn.Linear(model.config.hidden_size, num_classes)\n# lora_config = LoraConfig(\n#     r=8,\n#     lora_alpha=32,\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     target_modules=[\"query\", \"key\", \"value\"],  # Simplified common targets; update if needed\n# )\n# model = get_peft_model(model, lora_config)\n# model.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.405568Z","iopub.execute_input":"2025-05-18T04:58:49.406249Z","iopub.status.idle":"2025-05-18T04:58:49.415893Z","shell.execute_reply.started":"2025-05-18T04:58:49.406217Z","shell.execute_reply":"2025-05-18T04:58:49.415275Z"}},"outputs":[],"execution_count":490},{"cell_type":"code","source":"# print(\"Train dataset length:\", len(train_dataset))\n# print(\"Sample item:\", train_dataset[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.416601Z","iopub.execute_input":"2025-05-18T04:58:49.416833Z","iopub.status.idle":"2025-05-18T04:58:49.430159Z","shell.execute_reply.started":"2025-05-18T04:58:49.416814Z","shell.execute_reply":"2025-05-18T04:58:49.429575Z"}},"outputs":[],"execution_count":491},{"cell_type":"code","source":"# # === Training setup ===\n# training_args = TrainingArguments(\n#     output_dir=\"./vilt-lora-vqa\",\n#     per_device_train_batch_size=4,\n#     per_device_eval_batch_size=4,\n#     num_train_epochs=3,\n#     fp16=True,\n#     learning_rate=1e-4,\n#     logging_steps=50,\n#     remove_unused_columns=False,\n#     report_to=\"none\",\n# )\n\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=test_dataset,\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.430865Z","iopub.execute_input":"2025-05-18T04:58:49.431561Z","iopub.status.idle":"2025-05-18T04:58:49.440106Z","shell.execute_reply.started":"2025-05-18T04:58:49.431521Z","shell.execute_reply":"2025-05-18T04:58:49.439457Z"}},"outputs":[],"execution_count":492},{"cell_type":"code","source":"# # Define your training arguments\n# training_args = TrainingArguments(\n#     output_dir=\"./vilt-lora-vqa\",\n#     per_device_train_batch_size=4,\n#     per_device_eval_batch_size=4,\n#     num_train_epochs=3,\n#     fp16=True,\n#     learning_rate=1e-4,\n#     logging_steps=50,\n#     remove_unused_columns=False,\n#     report_to=\"none\",\n# )\n\n# # Initialize the Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,  # Pass the raw Dataset (not DataLoader)\n#     eval_dataset=test_dataset,    # Pass the raw Dataset (not DataLoader)\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.440666Z","iopub.execute_input":"2025-05-18T04:58:49.440833Z","iopub.status.idle":"2025-05-18T04:58:49.449782Z","shell.execute_reply.started":"2025-05-18T04:58:49.440819Z","shell.execute_reply":"2025-05-18T04:58:49.449167Z"}},"outputs":[],"execution_count":493},{"cell_type":"code","source":"# # === Train ===\n# train_results = trainer.train()\n# trainer.save_model(\"./vilt-lora-vqa-final\")\n\n# # === Evaluation ===\n# eval_results = trainer.evaluate()\n# print(f\"Final evaluation results: {eval_results}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.450386Z","iopub.execute_input":"2025-05-18T04:58:49.450573Z","iopub.status.idle":"2025-05-18T04:58:49.459727Z","shell.execute_reply.started":"2025-05-18T04:58:49.450551Z","shell.execute_reply":"2025-05-18T04:58:49.459138Z"}},"outputs":[],"execution_count":494},{"cell_type":"code","source":"# === Evaluation ===\neval_results = trainer.evaluate()\nprint(f\"Final evaluation results: {eval_results}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T04:58:49.460420Z","iopub.execute_input":"2025-05-18T04:58:49.461084Z","iopub.status.idle":"2025-05-18T05:01:21.726884Z","shell.execute_reply.started":"2025-05-18T04:58:49.461061Z","shell.execute_reply":"2025-05-18T05:01:21.726196Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='843' max='843' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [843/843 02:32]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final evaluation results: {'eval_runtime': 152.2522, 'eval_samples_per_second': 44.295, 'eval_steps_per_second': 5.537, 'epoch': 5.0}\n","output_type":"stream"}],"execution_count":495},{"cell_type":"code","source":"from PIL import Image, UnidentifiedImageError\nimport time\n# === Inference ===\npred_answers = []\nfailed_indices = []\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\nid2answer = {idx: ans for ans, idx in answer2id.items()}\n\nstart_time = time.time()\n\nfor idx, row in test_df.iterrows():\n    try:\n        image = Image.open(row['image_path']).convert('RGB')\n        image = image.resize(IMAGE_SIZE)\n    except (FileNotFoundError, UnidentifiedImageError, OSError) as e:\n        print(f\"Warning: Skipping image at index {idx}, error: {e}\")\n        pred_answers.append(\"error\")\n        failed_indices.append(idx)\n        continue\n\n    question = row['question']\n    \n    inputs = processor(\n        images=image,\n        text=question,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        truncation=True,\n        max_length=40\n    ).to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        pred_idx = logits.argmax(-1).item()\n        pred_answer = id2answer.get(pred_idx, \"unknown\")\n        pred_answers.append(pred_answer)\n\nend_time = time.time()\nprint(f\"Total inference time: {end_time - start_time:.2f} seconds\")\n\n# === Add predictions ===\ntest_df['predicted_answer'] = pred_answers\n\n# === Normalize for comparison ===\ndef normalize(text):\n    return str(text).strip().lower()\n\ntest_df['is_true'] = (test_df['predicted_answer'].apply(normalize) == test_df['answer'].apply(normalize)).astype(int)\n\n# === Report ===\nprint(f\"Correct predictions: {test_df['is_true'].sum()}\")\nprint(f\"Incorrect predictions: {(test_df['is_true'] == 0).sum()}\")\nprint(f\"Failed images: {len(failed_indices)}\")\nprint(f\"Accuracy: {test_df['is_true'].mean():.4f}\")\n\n# === Classification report ===\ntrue_labels = test_df['answer'].apply(normalize)\npred_labels = test_df['predicted_answer'].apply(normalize)\nprint(\"\\nClassification Report:\\n\", classification_report(true_labels, pred_labels, zero_division=0))\n\n# === Save results ===\ntest_df.to_csv(\"VILT_LORA_PREDICTIONS.csv\", index=False)\nprint(\"Saved results to VILT_LORA_PREDICTIONS.csv\")\n\n# === Visualize sample predictions ===\nprint(\"\\nSample Predictions:\")\nfor idx, row in test_df.head(5).iterrows():\n    print(f\"Question: {row['question']}\")\n    print(f\"True Answer: {row['answer']}\")\n    print(f\"Predicted: {row['predicted_answer']}\")\n    print(f\"Image: {row['image_path']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:01:21.727621Z","iopub.execute_input":"2025-05-18T05:01:21.727865Z","iopub.status.idle":"2025-05-18T05:04:31.090228Z","shell.execute_reply.started":"2025-05-18T05:01:21.727847Z","shell.execute_reply":"2025-05-18T05:04:31.089575Z"}},"outputs":[{"name":"stdout","text":"Total inference time: 189.08 seconds\nCorrect predictions: 4317\nIncorrect predictions: 2427\nFailed images: 0\nAccuracy: 0.6401\n\nClassification Report:\n                    precision    recall  f1-score   support\n\n               10       0.00      0.00      0.00         0\n              100       0.33      0.25      0.29         4\n               12       0.00      0.00      0.00         0\n              120       0.00      0.00      0.00         1\n               15       0.00      0.00      0.00         1\n              150       0.00      0.00      0.00         2\n               16       0.00      0.00      0.00         1\n               18       0.00      0.00      0.00         1\n                2       0.00      0.00      0.00         2\n               20       0.25      0.33      0.29         3\n              200       0.00      0.00      0.00         2\n               22       0.00      0.00      0.00         1\n               24       0.00      0.00      0.00         1\n               25       0.00      0.00      0.00         1\n               27       0.00      0.00      0.00         1\n                3       0.00      0.00      0.00         0\n               30       0.00      0.00      0.00         4\n              300       0.00      0.00      0.00         1\n               36       0.00      0.00      0.00         1\n               42       0.00      0.00      0.00         1\n               44       0.00      0.00      0.00         1\n               50       0.00      0.00      0.00         0\n              500       0.00      0.00      0.00         0\n                6       0.00      0.00      0.00         1\n               64       0.00      0.00      0.00         1\n                8       0.00      0.00      0.00         0\n                9       0.00      0.00      0.00         1\n               90       0.00      0.00      0.00         1\n         abstract       0.00      0.00      0.00         1\n           africa       0.00      0.00      0.00         1\n          almonds       0.00      0.00      0.00         3\n         aluminum       0.00      0.00      0.00         7\n           amazon       0.00      0.00      0.00         1\n            apple       0.00      0.00      0.00         0\n           apples       0.00      0.00      0.00         2\n           arrows       0.00      0.00      0.00         1\n              art       0.00      0.00      0.00         1\n                b       0.07      1.00      0.12         1\n             back       0.00      0.00      0.00         2\n       background       0.00      0.00      0.00         1\n         backpack       0.00      0.00      0.00         1\n              bag       0.31      0.53      0.39        36\n             bags       0.00      0.00      0.00        14\n             ball       0.43      0.38      0.40         8\n            balls       0.00      0.00      0.00         1\n           bamboo       0.00      0.00      0.00         0\n             band       0.00      0.00      0.00         3\n          bandana       0.00      0.00      0.00         1\n             bank       0.00      0.00      0.00         1\n              bar       0.57      0.40      0.47        20\n           basket       0.30      0.33      0.32         9\n          baskets       0.00      0.00      0.00         2\n            beads       0.00      0.00      0.00         1\n            beans       1.00      0.33      0.50         3\n             bear       0.09      0.50      0.15         2\n            beard       1.00      0.50      0.67         2\n            bears       0.00      0.00      0.00         0\n              bed       0.25      0.33      0.29         9\n             beef       0.00      0.00      0.00         1\n            beige       0.61      0.30      0.40        64\n             bell       0.00      0.00      0.00         1\n             belt       0.00      0.00      0.00         0\n            bench       0.62      0.71      0.67         7\n           bikini       0.00      0.00      0.00         0\n              bin       0.00      0.00      0.00         5\n             bird       1.00      0.50      0.67         2\n            birds       0.00      0.00      0.00         2\n            black       0.84      0.91      0.87       716\n          blanket       0.00      0.00      0.00         2\n          blender       0.25      0.50      0.33         4\n           blinds       0.00      0.00      0.00         1\n           blonde       1.00      1.00      1.00         1\n             blue       0.74      0.84      0.79       259\n      blueberries       0.00      0.00      0.00         1\n            board       0.00      0.00      0.00        10\n             boat       0.00      0.00      0.00         1\n             bone       0.00      0.00      0.00         1\n             book       0.00      0.00      0.00         0\n            books       0.00      0.00      0.00         0\n             boot       0.00      0.00      0.00         3\n            boots       0.33      0.50      0.40         2\n           bottle       0.51      0.78      0.62       103\n          bottles       0.60      0.33      0.43         9\n              bow       0.00      0.00      0.00         2\n             bowl       0.33      0.43      0.38         7\n            bowls       0.00      0.00      0.00         2\n              box       0.45      0.87      0.60        69\n            boxes       0.50      0.14      0.22         7\n              boy       0.00      0.00      0.00         6\n            brace       0.00      0.00      0.00         2\n         bracelet       1.00      0.25      0.40         8\n           branch       1.00      1.00      1.00         2\n            brass       0.45      0.23      0.30        22\n            bread       0.20      0.50      0.29         2\n            brick       0.00      0.00      0.00         0\n           bricks       0.00      0.00      0.00         1\n         broccoli       1.00      0.50      0.67         2\n           bronze       0.46      0.32      0.38        41\n            broom       0.00      0.00      0.00         2\n            brown       0.72      0.73      0.73       221\n            brush       0.60      0.60      0.60         5\n           bucket       0.43      1.00      0.60         3\n         building       0.00      0.00      0.00         0\n            bunny       0.00      0.00      0.00         1\n              bus       1.00      0.67      0.80         3\n        butterfly       0.00      0.00      0.00         1\n                c       0.00      0.00      0.00         2\n          cabinet       0.50      0.53      0.51        19\n           cactus       1.00      0.33      0.50         3\n             cage       0.00      0.00      0.00         1\n             cake       0.00      0.00      0.00         1\n           camera       0.33      0.50      0.40         2\n             camo       0.00      0.00      0.00         0\n       camouflage       1.00      0.75      0.86         4\n              can       0.00      0.00      0.00        14\n           candle       1.00      0.50      0.67         4\n            candy       0.00      0.00      0.00         0\n             cane       0.00      0.00      0.00         0\n           canopy       0.50      0.50      0.50         2\n       cantaloupe       0.00      0.00      0.00         2\n              cap       0.00      0.00      0.00         1\n              car       0.50      0.50      0.50         2\n        cardboard       1.00      0.78      0.88         9\n          carrots       0.25      1.00      0.40         1\n             cars       0.00      0.00      0.00         0\n             cart       0.00      0.00      0.00         2\n             case       0.88      0.96      0.92       550\n              cat       0.50      0.75      0.60        12\n            catch       0.00      0.00      0.00         2\n             cats       0.00      0.00      0.00         1\n               cd       0.00      0.00      0.00         1\n           cement       0.00      0.00      0.00         0\n          ceramic       0.00      0.00      0.00         1\n            chain       0.00      0.00      0.00         1\n            chair       0.65      0.72      0.68        76\n           chairs       1.00      0.50      0.67         4\n       chandelier       0.00      0.00      0.00         3\n        checkered       0.00      0.00      0.00         1\n          cheddar       1.00      1.00      1.00         1\n           cheese       0.00      0.00      0.00         3\n            chest       0.00      0.00      0.00         1\n          chevron       0.00      0.00      0.00         3\n            china       0.00      0.00      0.00         1\n          chinese       0.00      0.00      0.00         0\n            chips       0.00      0.00      0.00         2\n        chocolate       0.00      0.00      0.00         1\n           chrome       0.60      0.09      0.15        35\n         cinnamon       0.00      0.00      0.00         2\n           circle       0.50      0.12      0.20         8\n          circles       0.33      0.17      0.22         6\n          cleaner       0.00      0.00      0.00         1\n            clear       0.71      0.42      0.53        76\n             clip       0.00      0.00      0.00         1\n            clock       0.89      1.00      0.94         8\n            cloth       0.00      0.00      0.00         1\n          clothes       0.00      0.00      0.00         1\n           clouds       0.00      0.00      0.00         0\n             coat       0.00      0.00      0.00         1\n           coffee       0.00      0.00      0.00         6\n           collar       0.00      0.00      0.00         1\n         computer       0.00      0.00      0.00         0\n             cone       1.00      0.17      0.29        12\n            cones       0.00      0.00      0.00         1\n        container       0.33      0.10      0.15        20\n       controller       0.00      0.00      0.00         2\n             cook       0.00      0.00      0.00         1\n           cookie       0.00      0.00      0.00         0\n          cookies       0.00      0.00      0.00         2\n           cooler       0.00      0.00      0.00         1\n           copper       0.00      0.00      0.00        10\n             cord       0.00      0.00      0.00         1\n             corn       0.00      0.00      0.00         2\n           corner       0.00      0.00      0.00         1\n           cotton       0.80      0.67      0.73         6\n            couch       0.00      0.00      0.00         0\n           couple       0.56      0.83      0.67         6\n            cover       0.00      0.00      0.00        16\n            cream       0.00      0.00      0.00        13\n            cross       0.00      0.00      0.00         3\n            crown       1.00      1.00      1.00         1\n              cup       0.54      0.65      0.59        23\n          cupcake       0.00      0.00      0.00         0\n         cupcakes       0.00      0.00      0.00         2\n         curtains       1.00      0.40      0.57         5\n           curved       0.32      0.32      0.32        31\n          cushion       0.00      0.00      0.00         4\n    cutting board       0.00      0.00      0.00         0\n         cylinder       0.38      0.58      0.46        31\n                d       0.00      0.00      0.00         1\n              dad       0.00      0.00      0.00         1\n             dark       0.00      0.00      0.00         4\n       decoration       0.00      0.00      0.00         0\n             desk       0.45      0.33      0.38        15\n          diamond       1.00      0.67      0.80         3\n         diamonds       0.00      0.00      0.00         0\n             dock       0.00      0.00      0.00         2\n              dog       0.50      0.50      0.50         2\n             dome       0.00      0.00      0.00         5\n            donut       0.00      0.00      0.00         0\n           donuts       0.00      0.00      0.00         0\n             door       0.83      0.62      0.71         8\n             dots       1.00      0.20      0.33         5\n           drawer       0.00      0.00      0.00         4\n            dress       0.33      1.00      0.50         1\n          dresser       0.17      0.33      0.22         3\n            drink       0.00      0.00      0.00         1\n            drive       0.00      0.00      0.00         3\n              dry       0.00      0.00      0.00         2\n                e       0.00      0.00      0.00         2\n          earbuds       0.00      0.00      0.00         2\n          earring       0.00      0.00      0.00         2\n         earrings       0.87      0.87      0.87        31\n             eggs       0.00      0.00      0.00         5\n     eiffel tower       0.00      0.00      0.00         0\n         electric       0.00      0.00      0.00         1\n        enclosure       0.00      0.00      0.00         1\n          english       0.00      0.00      0.00         0\n                f       0.00      0.00      0.00         1\n           fabric       0.50      0.43      0.46        14\n             face       0.00      0.00      0.00         1\n              fan       0.33      0.50      0.40         2\n           faucet       0.86      0.67      0.75         9\n         feathers       0.00      0.00      0.00         1\n           feeder       0.00      0.00      0.00         1\n             feet       0.00      0.00      0.00         1\n             flat       0.00      0.00      0.00         0\n            flats       0.00      0.00      0.00         2\n            floor       0.00      0.00      0.00         0\n           floral       0.25      0.17      0.20         6\n            flour       0.00      0.00      0.00         2\n           flower       0.00      0.00      0.00         2\n          flowers       0.64      0.82      0.72        28\n             foam       0.00      0.00      0.00         0\n             foil       0.00      0.00      0.00         6\n             food       0.00      0.00      0.00         0\n             fork       0.25      0.33      0.29         3\n         fountain       0.00      0.00      0.00         1\n              fox       0.00      0.00      0.00         1\n            frame       0.37      0.44      0.40        25\n             free       0.00      0.00      0.00         1\n           french       0.00      0.00      0.00         1\n           fridge       0.00      0.00      0.00         1\n            fries       0.00      0.00      0.00         0\n             frog       0.00      0.00      0.00         1\n            fruit       0.00      0.00      0.00         0\n        furniture       0.00      0.00      0.00         1\n                g       0.00      0.00      0.00         3\n           garlic       0.00      0.00      0.00         2\n           gazebo       0.00      0.00      0.00         1\n           german       0.00      0.00      0.00         1\n           ginger       0.00      0.00      0.00         1\n          giraffe       0.00      0.00      0.00         0\n         giraffes       0.00      0.00      0.00         1\n             girl       0.29      0.25      0.27         8\n            girls       0.00      0.00      0.00         2\n            glass       0.76      0.58      0.66        72\n          glasses       0.25      0.33      0.29         3\n            globe       0.00      0.00      0.00         4\n            glove       0.25      1.00      0.40         1\n           gloves       0.75      0.30      0.43        10\n             gold       0.60      0.57      0.58        53\n            goose       0.00      0.00      0.00         1\n            grape       0.00      0.00      0.00         2\n            grass       0.00      0.00      0.00         0\n             gray       0.45      0.22      0.29        64\n            green       0.76      0.78      0.77       153\n              gun       0.00      0.00      0.00         1\n                h       0.00      0.00      0.00         4\n       hair dryer       0.00      0.00      0.00         0\n          hammock       0.00      0.00      0.00         1\n             hand       0.00      0.00      0.00         1\n           handle       0.53      0.91      0.67        64\n            hands       0.00      0.00      0.00         1\n           hanger       0.00      0.00      0.00         9\n            happy       0.00      0.00      0.00         1\n             hard       0.00      0.00      0.00         3\n              hat       0.00      0.00      0.00         1\n             hats       0.00      0.00      0.00         0\n             head       0.00      0.00      0.00         1\n       headphones       1.00      1.00      1.00         8\n            heart       0.43      0.75      0.55        20\n           hearts       0.50      0.47      0.48        15\n           heater       0.00      0.00      0.00         3\n          hexagon       0.00      0.00      0.00         3\n            honey       0.20      1.00      0.33         1\n             hood       0.00      0.00      0.00         1\n           hoodie       0.00      0.00      0.00         2\n            house       1.00      1.00      1.00         1\n                i       0.00      0.00      0.00         3\n              ice       0.00      0.00      0.00         0\n      information       0.00      0.00      0.00         1\n            ivory       0.00      0.00      0.00         6\n                j       0.00      0.00      0.00         2\n          jackets       0.00      0.00      0.00         0\n         japanese       0.00      0.00      0.00         1\n              jar       0.00      0.00      0.00        13\n            jeans       0.00      0.00      0.00         2\n            jelly       0.00      0.00      0.00         2\n            juice       0.00      0.00      0.00         8\n                k       0.00      0.00      0.00         2\n           kettle       1.00      0.67      0.80         3\n         keyboard       0.00      0.00      0.00         2\n             keys       0.00      0.00      0.00         1\n             king       0.00      0.00      0.00         2\n          kissing       0.00      0.00      0.00         1\n           kitten       1.00      0.50      0.67         2\n            knife       0.00      0.00      0.00         1\n           knives       1.00      1.00      1.00         1\n                l       0.14      0.50      0.22         2\n             lady       0.00      0.00      0.00         0\n             lamp       0.80      0.82      0.81        82\n           laptop       0.00      0.00      0.00         1\n             leaf       0.00      0.00      0.00         5\n            leash       0.00      0.00      0.00         1\n          leather       0.64      0.76      0.69        33\n           leaves       1.00      0.40      0.57         5\n              leg       0.00      0.00      0.00         2\n             legs       0.00      0.00      0.00         2\n            lemon       0.00      0.00      0.00         0\n          leopard       0.00      0.00      0.00         1\n              lid       0.00      0.00      0.00         2\n            light       0.42      0.41      0.41        32\n           lights       0.50      0.12      0.20         8\n             lily       0.00      0.00      0.00         1\n             lime       0.00      0.00      0.00         1\n             lion       0.00      0.00      0.00         2\n          lobster       0.00      0.00      0.00         1\n             logo       0.00      0.00      0.00         0\n              lot       0.00      0.00      0.00         0\n           lotion       0.00      0.00      0.00         4\n             love       0.50      0.20      0.29        10\n                m       0.00      0.00      0.00         4\n          machine       0.00      0.00      0.00         2\n           makeup       0.00      0.00      0.00         0\n              man       1.00      0.50      0.67         2\n             many       0.50      0.20      0.29         5\n           marble       1.00      0.33      0.50         6\n           marker       0.43      0.38      0.40         8\n           maroon       0.00      0.00      0.00         1\n          married       0.00      0.00      0.00         0\n             mask       0.00      0.00      0.00         9\n              mat       0.00      0.00      0.00         5\n         mattress       0.00      0.00      0.00         5\n               me       0.00      0.00      0.00         3\n           medium       0.00      0.00      0.00         1\n             menu       0.00      0.00      0.00         0\n             meow       1.00      0.40      0.57         5\n            metal       0.43      0.56      0.49        55\n            meter       1.00      1.00      1.00         3\n       microphone       0.00      0.00      0.00         1\n        microwave       0.00      0.00      0.00         0\n           middle       0.00      0.00      0.00         0\n             milk       0.42      0.53      0.47        15\n             mint       1.00      0.50      0.67         2\n           mirror       0.33      0.29      0.31         7\n            mixer       0.00      0.00      0.00         1\n          monitor       0.00      0.00      0.00         2\n          monster       0.00      0.00      0.00         1\n             moon       0.00      0.00      0.00         5\n           mosaic       0.00      0.00      0.00         0\n       motorcycle       0.00      0.00      0.00         0\n         mountain       0.00      0.00      0.00         1\n        mountains       0.00      0.00      0.00         1\n            mouse       0.50      1.00      0.67         1\n        mouthwash       0.00      0.00      0.00         1\n              mug       0.00      0.00      0.00         2\n         mushroom       0.00      0.00      0.00         1\n          mustard       0.00      0.00      0.00         1\n                n       0.50      0.25      0.33         4\n           napkin       0.00      0.00      0.00         1\n          napkins       0.00      0.00      0.00         1\n          natural       0.00      0.00      0.00         2\n             navy       0.00      0.00      0.00        10\n         necklace       0.74      1.00      0.85        26\n              net       0.67      1.00      0.80         2\n               no       0.50      0.19      0.27        27\n          noodles       0.00      0.00      0.00         1\n         notebook       0.00      0.00      0.00         5\n          nothing       0.00      0.00      0.00         0\n             nuts       0.00      0.00      0.00         2\n                o       0.00      0.00      0.00         1\n              oak       0.00      0.00      0.00         7\n              oil       0.06      0.10      0.07        10\n           olives       0.00      0.00      0.00         4\n           onions       0.00      0.00      0.00         1\n           opaque       1.00      0.64      0.78        22\n           orange       0.71      0.64      0.67        83\norange and yellow       0.00      0.00      0.00         0\n          oranges       0.00      0.00      0.00         0\n          organic       0.00      0.00      0.00         3\n          ottoman       0.00      0.00      0.00         6\n             oval       0.07      0.08      0.08        24\n             oven       0.00      0.00      0.00         3\n              owl       0.00      0.00      0.00         0\n                p       0.00      0.00      0.00         2\n         pacifier       0.00      0.00      0.00         1\n            paint       0.00      0.00      0.00         1\n          paisley       0.00      0.00      0.00         1\n              pan       0.40      0.55      0.46        11\n            panda       0.00      0.00      0.00         8\n             pans       0.00      0.00      0.00         1\n            pants       0.00      0.00      0.00         1\n            paper       0.33      0.20      0.25        10\n           papers       0.00      0.00      0.00         0\n           parrot       1.00      1.00      1.00         3\n          parsley       0.00      0.00      0.00         1\n            pasta       0.00      0.00      0.00         6\n               pc       0.00      0.00      0.00         3\n            peace       0.00      0.00      0.00         1\n            peach       0.00      0.00      0.00         2\n          peaches       0.00      0.00      0.00         1\n          peanuts       0.00      0.00      0.00         1\n             pear       0.00      0.00      0.00         2\n             peas       0.00      0.00      0.00         4\n          pelican       0.00      0.00      0.00         1\n              pen       0.33      0.60      0.43         5\n           pencil       0.00      0.00      0.00         5\n           people       0.00      0.00      0.00         0\n           pepper       0.00      0.00      0.00         2\n           person       0.00      0.00      0.00         0\n            phone       0.33      0.50      0.40         2\n          pickles       0.00      0.00      0.00         1\n          picture       0.00      0.00      0.00         0\n         pictures       0.00      0.00      0.00         0\n              pig       0.00      0.00      0.00         4\n           pillow       0.55      0.74      0.63        23\n          pillows       0.00      0.00      0.00         0\n             pine       0.00      0.00      0.00         1\n        pineapple       1.00      0.20      0.33         5\n             pink       0.85      0.87      0.86       176\n             pipe       1.00      1.00      1.00         1\n          pitcher       0.00      0.00      0.00         3\n            pizza       1.00      1.00      1.00         1\n            plain       0.00      0.00      0.00         0\n            plane       0.00      0.00      0.00         0\n            plant       0.00      0.00      0.00         0\n          planter       0.00      0.00      0.00         6\n           plants       0.00      0.00      0.00         0\n          plastic       0.49      0.74      0.59        53\n            plate       0.36      0.44      0.40         9\n           plates       0.50      0.50      0.50         2\n           player       0.00      0.00      0.00         1\n          plunger       0.00      0.00      0.00         1\n           pocket       0.00      0.00      0.00         2\n             pole       0.00      0.00      0.00         1\n            poles       0.00      0.00      0.00         0\n        polka dot       0.00      0.00      0.00         0\n        porcelain       0.00      0.00      0.00         1\n             pork       0.00      0.00      0.00         1\n              pot       0.25      1.00      0.40         3\n           potato       0.00      0.00      0.00         0\n         potatoes       0.00      0.00      0.00         1\n            print       0.00      0.00      0.00         2\n             pull       0.00      0.00      0.00         1\n            puppy       0.00      0.00      0.00         1\n           purple       0.75      0.81      0.78        62\n            purse       0.00      0.00      0.00         0\n            queen       0.50      0.60      0.55         5\n                r       0.00      0.00      0.00         1\n           rabbit       0.00      0.00      0.00         1\n             rack       0.00      0.00      0.00         8\n          rainbow       0.00      0.00      0.00         2\n         recliner       0.00      0.00      0.00         1\n        rectangle       0.63      0.90      0.74       101\n       rectangles       0.00      0.00      0.00         2\n              red       0.78      0.85      0.82       153\n             rice       0.33      0.33      0.33         6\n            right       1.00      0.50      0.67         2\n             ring       0.59      0.74      0.66        27\n             rock       0.00      0.00      0.00         1\n            rocks       0.00      0.00      0.00         0\n             roll       0.00      0.00      0.00         9\n             rope       0.00      0.00      0.00         1\n             rose       0.00      0.00      0.00         5\n            roses       0.50      0.14      0.22         7\n            round       0.56      0.82      0.67       137\n           rubber       0.00      0.00      0.00         7\n              rug       0.00      0.00      0.00         2\n             rust       0.00      0.00      0.00         1\n                s       1.00      0.25      0.40         4\n              sad       0.00      0.00      0.00         1\n             safe       0.00      0.00      0.00         6\n            salad       0.67      0.50      0.57         4\n           salmon       0.00      0.00      0.00         1\n             salt       0.00      0.00      0.00         5\n          sandals       0.00      0.00      0.00         9\n         sandwich       0.00      0.00      0.00         1\n            sauce       0.00      0.00      0.00         5\n            scale       0.00      0.00      0.00         1\n            scarf       0.50      1.00      0.67         1\n         scissors       0.00      0.00      0.00         1\n           screen       0.00      0.00      0.00         2\n           sesame       0.00      0.00      0.00         0\n            shade       0.00      0.00      0.00         4\n          shampoo       0.00      0.00      0.00         1\n            shark       0.00      0.00      0.00         1\n            sheet       0.12      0.14      0.13         7\n           sheets       0.00      0.00      0.00        12\n            shelf       0.20      0.27      0.23        15\n          shelves       0.50      0.25      0.33         4\n            shirt       0.00      0.00      0.00         1\n             shoe       0.25      0.17      0.20         6\n            shoes       0.67      0.97      0.80        36\n             shop       0.00      0.00      0.00         1\n           shower       0.00      0.00      0.00         1\n             sign       0.00      0.00      0.00         0\n             silk       0.00      0.00      0.00         3\n           silver       0.53      0.56      0.54       130\n             sink       0.00      0.00      0.00         0\n            skull       0.29      0.25      0.27         8\n           sleeve       0.00      0.00      0.00         2\n            slide       0.00      0.00      0.00         2\n         slippers       0.00      0.00      0.00         1\n            small       0.00      0.00      0.00         0\n            smile       0.00      0.00      0.00         3\n      smiley face       0.00      0.00      0.00         0\n           smooth       1.00      0.50      0.67         2\n             soap       0.00      0.00      0.00         4\n            socks       0.00      0.00      0.00         0\n             soda       0.00      0.00      0.00         0\n             sofa       0.65      0.65      0.65        23\n             soft       0.50      0.33      0.40         3\n             soup       1.00      0.33      0.50         3\n            space       0.00      0.00      0.00         3\n          spatula       0.00      0.00      0.00         2\n          speaker       0.00      0.00      0.00         4\n           spices       0.00      0.00      0.00         0\n          spinach       0.33      1.00      0.50         1\n           spiral       0.00      0.00      0.00         2\n            spoon       0.71      0.83      0.77         6\n            spots       0.00      0.00      0.00         1\n           square       0.47      0.39      0.43        64\n          squares       0.00      0.00      0.00         1\n         squirrel       0.00      0.00      0.00         1\n            stand       0.14      0.08      0.11        12\n             star       0.00      0.00      0.00         2\n            stars       0.30      0.46      0.36        13\n            steel       0.60      0.22      0.32        27\n            stick       0.00      0.00      0.00         1\n           sticks       0.00      0.00      0.00         4\n            stool       0.33      0.11      0.17         9\n             stop       0.00      0.00      0.00         3\n            strap       0.00      0.00      0.00         4\n            straw       0.00      0.00      0.00         2\n     strawberries       0.00      0.00      0.00         0\n       strawberry       0.00      0.00      0.00         4\n           string       0.00      0.00      0.00         0\n          striped       0.00      0.00      0.00         1\n          stripes       0.38      0.50      0.43        10\n            sugar       0.00      0.00      0.00         6\n         suitcase       0.67      1.00      0.80         2\n              sun       1.00      1.00      1.00         1\n        sunflower       0.00      0.00      0.00         0\n       sunglasses       0.80      0.67      0.73         6\n           sunset       0.00      0.00      0.00         1\n            sweet       0.00      0.00      0.00         0\n           swirls       0.00      0.00      0.00         0\n            syrup       0.00      0.00      0.00         2\n                t       0.00      0.00      0.00         4\n            table       0.71      0.71      0.71        77\n           tablet       0.00      0.00      0.00         1\n              tag       0.00      0.00      0.00         0\n             tall       0.00      0.00      0.00         1\n              tan       0.00      0.00      0.00         1\n             tape       0.00      0.00      0.00         4\n              tea       0.00      0.00      0.00         6\n             teal       0.40      0.10      0.16        20\n       teddy bear       0.00      0.00      0.00         0\n             tent       0.00      0.00      0.00         1\n              tie       0.50      1.00      0.67         1\n            tiger       0.00      0.00      0.00         3\n             tire       0.00      0.00      0.00         0\n           tissue       0.00      0.00      0.00         1\n          toaster       0.00      0.00      0.00         3\n            tools       0.00      0.00      0.00         3\n       toothbrush       0.00      0.00      0.00         2\n       toothpaste       0.00      0.00      0.00         0\n            towel       0.17      0.20      0.18         5\n            tower       0.67      0.44      0.53         9\n        trash can       0.00      0.00      0.00         0\n             tray       0.00      0.00      0.00         5\n             tree       1.00      0.12      0.22         8\n            trees       0.33      1.00      0.50         1\n         triangle       0.14      0.14      0.14         7\n        triangles       0.00      0.00      0.00         3\n           tripod       0.50      0.50      0.50         2\n             tube       0.00      0.00      0.00         7\n            tulip       0.00      0.00      0.00         1\n         umbrella       0.50      0.40      0.44         5\n                v       0.00      0.00      0.00         3\n          vanilla       0.00      0.00      0.00         1\n             vase       0.00      0.00      0.00         1\n        vegetable       0.00      0.00      0.00         0\n       vegetables       0.00      0.00      0.00         1\n         vertical       0.00      0.00      0.00         0\n             vest       0.00      0.00      0.00         1\n                w       0.50      1.00      0.67         1\n           wallet       0.00      0.00      0.00         5\n            water       0.12      0.12      0.12         8\n       watermelon       0.00      0.00      0.00         2\n             wave       0.00      0.00      0.00         4\n            waves       0.00      0.00      0.00         2\n          wedding       0.00      0.00      0.00         0\n            whale       0.00      0.00      0.00         2\n            wheat       0.00      0.00      0.00         3\n            wheel       0.00      0.00      0.00         2\n           wheels       0.00      0.00      0.00         3\n            white       0.71      0.85      0.78       500\n           wicker       0.00      0.00      0.00         4\n           window       0.00      0.00      0.00         0\n             wine       1.00      0.33      0.50         3\n             wire       0.00      0.00      0.00         2\n            woman       0.20      0.33      0.25         3\n             wood       0.68      0.82      0.75        93\n             wool       0.00      0.00      0.00         0\n            words       0.00      0.00      0.00         0\n          writing       0.00      0.00      0.00         0\n                x       0.00      0.00      0.00         1\n                y       0.00      0.00      0.00         2\n             yarn       0.00      0.00      0.00         0\n           yellow       0.76      0.76      0.76        98\n              yes       0.79      0.97      0.88       151\n           yogurt       1.00      0.20      0.33         5\n            young       0.00      0.00      0.00         1\n           zipper       0.00      0.00      0.00         4\n\n         accuracy                           0.64      6744\n        macro avg       0.18      0.17      0.16      6744\n     weighted avg       0.60      0.64      0.61      6744\n\nSaved results to VILT_LORA_PREDICTIONS.csv\n\nSample Predictions:\nQuestion: What is the box made of?\nTrue Answer: cardboard\nPredicted: cardboard\nImage: /kaggle/input/abo-dataset/images/small/10/10e61e07.jpg\n\nQuestion: What is the box made of?\nTrue Answer: cardboard\nPredicted: cardboard\nImage: /kaggle/input/abo-dataset/images/small/4e/4efbdd43.jpg\n\nQuestion: What is the main color of the Sphinx?\nTrue Answer: brown\nPredicted: blue\nImage: /kaggle/input/abo-dataset/images/small/7e/7e8da80a.jpg\n\nQuestion: What is in the image?\nTrue Answer: chair\nPredicted: chair\nImage: /kaggle/input/abo-dataset/images/small/ce/ced791d7.jpg\n\nQuestion: What color is it?\nTrue Answer: brown\nPredicted: brown\nImage: /kaggle/input/abo-dataset/images/small/16/160da766.jpg\n\n","output_type":"stream"}],"execution_count":496},{"cell_type":"code","source":"# Check how many answers are unknown (-1 index)\nunknown_count = sum([1 for a in test_df['answer'] if answer2id.get(a, -1) == -1])\nprint(f\"Unknown answers in test set: {unknown_count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:04:31.091071Z","iopub.execute_input":"2025-05-18T05:04:31.091766Z","iopub.status.idle":"2025-05-18T05:04:31.097300Z","shell.execute_reply.started":"2025-05-18T05:04:31.091746Z","shell.execute_reply":"2025-05-18T05:04:31.096498Z"}},"outputs":[{"name":"stdout","text":"Unknown answers in test set: 0\n","output_type":"stream"}],"execution_count":497},{"cell_type":"code","source":"import os\n\nfor f in os.listdir(\"/kaggle/working/\"):\n    print(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:04:31.098070Z","iopub.execute_input":"2025-05-18T05:04:31.098324Z","iopub.status.idle":"2025-05-18T05:04:31.108148Z","shell.execute_reply.started":"2025-05-18T05:04:31.098301Z","shell.execute_reply":"2025-05-18T05:04:31.107594Z"}},"outputs":[{"name":"stdout","text":"VILT_LORA_PREDICTIONS.csv\nvilt-lora-vqa-final\nvilt-lora-vqa\nvilt-lora-vqa-final.zip\n.virtual_documents\n","output_type":"stream"}],"execution_count":498},{"cell_type":"code","source":"!zip -r /kaggle/working/vilt-lora-vqa-final.zip /kaggle/working/vilt-lora-vqa-final\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:04:31.108894Z","iopub.execute_input":"2025-05-18T05:04:31.109145Z","iopub.status.idle":"2025-05-18T05:04:31.478248Z","shell.execute_reply.started":"2025-05-18T05:04:31.109124Z","shell.execute_reply":"2025-05-18T05:04:31.477371Z"}},"outputs":[{"name":"stdout","text":"updating: kaggle/working/vilt-lora-vqa-final/ (stored 0%)\nupdating: kaggle/working/vilt-lora-vqa-final/adapter_model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 7%)\nupdating: kaggle/working/vilt-lora-vqa-final/README.md (deflated 66%)\nupdating: kaggle/working/vilt-lora-vqa-final/training_args.bin (deflated 52%)\nupdating: kaggle/working/vilt-lora-vqa-final/adapter_config.json (deflated 53%)\n","output_type":"stream"}],"execution_count":499},{"cell_type":"code","source":"from collections import Counter\nimport matplotlib.pyplot as plt\n\ncounts = Counter(train_df['answer'])\nmost_common = counts.most_common(10)\nprint(\"Most common answers:\", most_common)\n\n# Optional: visualize\nplt.bar(*zip(*most_common))\nplt.xticks(rotation=45)\nplt.title(\"Top 10 Frequent Answers in Training Set\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T05:04:31.479548Z","iopub.execute_input":"2025-05-18T05:04:31.479794Z","iopub.status.idle":"2025-05-18T05:04:31.646464Z","shell.execute_reply.started":"2025-05-18T05:04:31.479764Z","shell.execute_reply":"2025-05-18T05:04:31.645749Z"}},"outputs":[{"name":"stdout","text":"Most common answers: [('black', 2601), ('case', 2358), ('white', 2285), ('blue', 1109), ('brown', 891), ('pink', 678), ('red', 619), ('yes', 578), ('green', 536), ('round', 533)]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":500}]}