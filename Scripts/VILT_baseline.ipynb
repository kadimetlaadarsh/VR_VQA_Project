{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11782776,"sourceType":"datasetVersion","datasetId":7397658},{"sourceId":11783144,"sourceType":"datasetVersion","datasetId":7397934}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:48:18.306480Z","iopub.execute_input":"2025-05-14T01:48:18.307056Z","iopub.status.idle":"2025-05-14T01:48:18.311403Z","shell.execute_reply.started":"2025-05-14T01:48:18.307010Z","shell.execute_reply":"2025-05-14T01:48:18.310492Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom transformers import ViltProcessor, ViltForQuestionAnswering\n\n# === Normalization Utilities ===\nnumber_map = {\n    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n    \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\"\n}\n\ndef normalize_answer(ans):\n    ans = str(ans).strip().lower()\n    if ans in number_map:\n        return number_map[ans]\n    if ans.isdigit():\n        return ans\n    if ans in [\"yes\", \"no\"]:\n        return ans\n    return ans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:48:18.312792Z","iopub.execute_input":"2025-05-14T01:48:18.313020Z","iopub.status.idle":"2025-05-14T01:48:59.161528Z","shell.execute_reply.started":"2025-05-14T01:48:18.312994Z","shell.execute_reply":"2025-05-14T01:48:59.160955Z"}},"outputs":[{"name":"stderr","text":"2025-05-14 01:48:42.210047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747187322.688182      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747187322.825857      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# === Model Setup ===\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nmodel = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\").eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)1qs`d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:48:59.162177Z","iopub.execute_input":"2025-05-14T01:48:59.162611Z","iopub.status.idle":"2025-05-14T01:49:04.681320Z","shell.execute_reply.started":"2025-05-14T01:48:59.162593Z","shell.execute_reply":"2025-05-14T01:49:04.680529Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97da1eb54b9044b39cd86fde350a43bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05288ac65bf648938f7aef77b71c2b92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf442bd01c244169ea67490a9f8aa9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f52086cb10647f3b095c2ff6efabebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd66ada9ed1c48689722ce78efd6100c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/136k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec311fece744443916611499da07dad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/470M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4047e934e4434e1db4849c177c07650d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/470M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49bd777bb1ff426088c092194dea8c78"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"ViltForQuestionAnswering(\n  (vilt): ViltModel(\n    (embeddings): ViltEmbeddings(\n      (text_embeddings): TextEmbeddings(\n        (word_embeddings): Embedding(30522, 768)\n        (position_embeddings): Embedding(40, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (patch_embeddings): ViltPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n      )\n      (token_type_embeddings): Embedding(2, 768)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViltEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViltLayer(\n          (attention): ViltAttention(\n            (attention): ViltSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViltSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViltIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViltOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViltPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=768, out_features=1536, bias=True)\n    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=1536, out_features=3129, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# === Load Dataset ===\ndf = pd.read_csv(\"/kaggle/input/merged-vqa/merged_vqa_dataset_output.csv\")\nBASE_IMAGE_DIR = \"/kaggle/input/abo-dataset/images/small\"\n\nall_results = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:49:04.683093Z","iopub.execute_input":"2025-05-14T01:49:04.683389Z","iopub.status.idle":"2025-05-14T01:49:04.783436Z","shell.execute_reply.started":"2025-05-14T01:49:04.683368Z","shell.execute_reply":"2025-05-14T01:49:04.782641Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# === Main Loop ===\nfor idx, row in df.iterrows():\n    full_image_path = os.path.join(BASE_IMAGE_DIR, str(row['path']))\n    try:\n        image = Image.open(full_image_path).convert('RGB')\n    except Exception as e:\n        print(f\"Failed to load image {full_image_path}: {e}\")\n        continue\n\n    for q_col, a_col in zip(['q1', 'q2', 'q3'], ['a1', 'a2', 'a3']):\n        question = row[q_col]\n        gt_answer = row[a_col]\n\n        try:\n            inputs = processor(image, question, return_tensors=\"pt\").to(device)\n            with torch.no_grad():\n                outputs = model(**inputs)\n                logits = outputs.logits\n                pred_idx = logits.argmax(-1).item()\n                pred_answer = model.config.id2label[pred_idx]\n        except Exception as e:\n            print(f\"Failed to predict for {full_image_path} / Q: {question} — {e}\")\n            pred_answer = \"error\"\n\n        # Normalize for comparison\n        gt_answer_norm = normalize_answer(gt_answer)\n        pred_answer_norm = normalize_answer(pred_answer)\n        is_correct = (pred_answer_norm == gt_answer_norm)\n\n        all_results.append({\n            \"path\": row['path'],\n            \"question\": question,\n            \"gt_answer\": gt_answer,\n            \"pred_answer\": pred_answer,\n            \"is_correct\": is_correct\n        })\n\n    if idx % 100 == 0:\n        print(f\"Processed {idx+1}/{len(df)} images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:49:04.784521Z","iopub.execute_input":"2025-05-14T01:49:04.784799Z","iopub.status.idle":"2025-05-14T02:09:10.814999Z","shell.execute_reply.started":"2025-05-14T01:49:04.784775Z","shell.execute_reply":"2025-05-14T02:09:10.814202Z"}},"outputs":[{"name":"stdout","text":"Processed 1/15368 images\nProcessed 101/15368 images\nProcessed 201/15368 images\nProcessed 301/15368 images\nProcessed 401/15368 images\nProcessed 501/15368 images\nProcessed 601/15368 images\nProcessed 701/15368 images\nProcessed 801/15368 images\nProcessed 901/15368 images\nProcessed 1001/15368 images\nFailed to predict for /kaggle/input/abo-dataset/images/small/5f/5f35a4d0.jpg / Q: What is in the image? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/5f/5f35a4d0.jpg / Q: What color is it? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/5f/5f35a4d0.jpg / Q: What is its shape? — height and width must be > 0\nProcessed 1101/15368 images\nProcessed 1201/15368 images\nProcessed 1301/15368 images\nProcessed 1401/15368 images\nProcessed 1501/15368 images\nProcessed 1601/15368 images\nProcessed 1701/15368 images\nProcessed 1801/15368 images\nProcessed 1901/15368 images\nProcessed 2001/15368 images\nProcessed 2101/15368 images\nProcessed 2201/15368 images\nProcessed 2301/15368 images\nProcessed 2401/15368 images\nProcessed 2501/15368 images\nProcessed 2601/15368 images\nProcessed 2701/15368 images\nProcessed 2801/15368 images\nProcessed 2901/15368 images\nProcessed 3001/15368 images\nProcessed 3101/15368 images\nProcessed 3201/15368 images\nProcessed 3301/15368 images\nProcessed 3401/15368 images\nProcessed 3501/15368 images\nProcessed 3601/15368 images\nProcessed 3701/15368 images\nProcessed 3801/15368 images\nProcessed 3901/15368 images\nProcessed 4001/15368 images\nProcessed 4101/15368 images\nProcessed 4201/15368 images\nProcessed 4301/15368 images\nProcessed 4401/15368 images\nProcessed 4501/15368 images\nProcessed 4601/15368 images\nProcessed 4701/15368 images\nProcessed 4801/15368 images\nProcessed 4901/15368 images\nProcessed 5001/15368 images\nProcessed 5101/15368 images\nProcessed 5201/15368 images\nProcessed 5301/15368 images\nProcessed 5401/15368 images\nProcessed 5501/15368 images\nProcessed 5601/15368 images\nProcessed 5701/15368 images\nProcessed 5801/15368 images\nProcessed 5901/15368 images\nProcessed 6001/15368 images\nProcessed 6101/15368 images\nProcessed 6201/15368 images\nProcessed 6301/15368 images\nProcessed 6401/15368 images\nProcessed 6501/15368 images\nProcessed 6601/15368 images\nProcessed 6701/15368 images\nProcessed 6801/15368 images\nProcessed 6901/15368 images\nProcessed 7001/15368 images\nProcessed 7101/15368 images\nProcessed 7201/15368 images\nProcessed 7301/15368 images\nFailed to predict for /kaggle/input/abo-dataset/images/small/0a/0a067ad5.jpg / Q: What is depicted? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/0a/0a067ad5.jpg / Q: What color is it? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/0a/0a067ad5.jpg / Q: What is the shape? — height and width must be > 0\nProcessed 7401/15368 images\nProcessed 7501/15368 images\nProcessed 7601/15368 images\nProcessed 7701/15368 images\nProcessed 7801/15368 images\nProcessed 7901/15368 images\nFailed to predict for /kaggle/input/abo-dataset/images/small/80/80cc6333.jpg / Q: What is shown? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/80/80cc6333.jpg / Q: What color is it? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/80/80cc6333.jpg / Q: What is its shape? — height and width must be > 0\nProcessed 8001/15368 images\nProcessed 8101/15368 images\nProcessed 8201/15368 images\nProcessed 8301/15368 images\nProcessed 8401/15368 images\nProcessed 8501/15368 images\nProcessed 8601/15368 images\nProcessed 8701/15368 images\nProcessed 8801/15368 images\nProcessed 8901/15368 images\nFailed to predict for /kaggle/input/abo-dataset/images/small/b9/b9d4509d.jpg / Q: What is depicted? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/b9/b9d4509d.jpg / Q: What color is it? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/b9/b9d4509d.jpg / Q: What is its shape? — height and width must be > 0\nProcessed 9001/15368 images\nFailed to predict for /kaggle/input/abo-dataset/images/small/6f/6fc51d79.jpg / Q: What is listed as an ingredient? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/6f/6fc51d79.jpg / Q: What is one word describing the ingredients? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/6f/6fc51d79.jpg / Q: What else is listed as an ingredient? — height and width must be > 0\nProcessed 9101/15368 images\nProcessed 9201/15368 images\nProcessed 9301/15368 images\nFailed to predict for /kaggle/input/abo-dataset/images/small/74/74ec6c98.jpg / Q: What is depicted? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/74/74ec6c98.jpg / Q: What color is it? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/74/74ec6c98.jpg / Q: What shape is the object? — height and width must be > 0\nProcessed 9401/15368 images\nProcessed 9501/15368 images\nProcessed 9601/15368 images\nProcessed 9701/15368 images\nProcessed 9801/15368 images\nProcessed 9901/15368 images\nFailed to predict for /kaggle/input/abo-dataset/images/small/5e/5e93d8b7.jpg / Q: What is in the image? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/5e/5e93d8b7.jpg / Q: What color is it? — height and width must be > 0\nFailed to predict for /kaggle/input/abo-dataset/images/small/5e/5e93d8b7.jpg / Q: Is it opaque? — height and width must be > 0\nProcessed 10001/15368 images\nProcessed 10101/15368 images\nProcessed 10201/15368 images\nProcessed 10301/15368 images\nProcessed 10401/15368 images\nProcessed 10501/15368 images\nProcessed 10601/15368 images\nProcessed 10701/15368 images\nProcessed 10801/15368 images\nProcessed 10901/15368 images\nProcessed 11001/15368 images\nProcessed 11101/15368 images\nProcessed 11201/15368 images\nProcessed 11301/15368 images\nProcessed 11401/15368 images\nProcessed 11501/15368 images\nProcessed 11601/15368 images\nProcessed 11701/15368 images\nProcessed 11801/15368 images\nProcessed 11901/15368 images\nProcessed 12001/15368 images\nProcessed 12101/15368 images\nProcessed 12201/15368 images\nProcessed 12301/15368 images\nProcessed 12401/15368 images\nProcessed 12501/15368 images\nProcessed 12601/15368 images\nProcessed 12701/15368 images\nProcessed 12801/15368 images\nProcessed 12901/15368 images\nProcessed 13001/15368 images\nProcessed 13101/15368 images\nProcessed 13201/15368 images\nProcessed 13301/15368 images\nProcessed 13401/15368 images\nProcessed 13501/15368 images\nProcessed 13601/15368 images\nProcessed 13701/15368 images\nProcessed 13801/15368 images\nProcessed 13901/15368 images\nProcessed 14001/15368 images\nProcessed 14101/15368 images\nProcessed 14201/15368 images\nProcessed 14301/15368 images\nProcessed 14401/15368 images\nProcessed 14501/15368 images\nProcessed 14601/15368 images\nProcessed 14701/15368 images\nProcessed 14801/15368 images\nProcessed 14901/15368 images\nProcessed 15001/15368 images\nProcessed 15101/15368 images\nProcessed 15201/15368 images\nProcessed 15301/15368 images\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# === Final Evaluation ===\nresults_df = pd.DataFrame(all_results)\naccuracy = results_df['is_correct'].mean()\nprint(f\"\\n✅ ViLBERT Baseline Accuracy: {accuracy:.3f}\")\n\n# === Save Output ===\nresults_df.to_csv(\"vilbert_vqa_baseline_results.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:09:10.816174Z","iopub.execute_input":"2025-05-14T02:09:10.816475Z","iopub.status.idle":"2025-05-14T02:09:11.010269Z","shell.execute_reply.started":"2025-05-14T02:09:10.816452Z","shell.execute_reply":"2025-05-14T02:09:11.009490Z"}},"outputs":[{"name":"stdout","text":"\n✅ ViLBERT Baseline Accuracy: 0.258\n","output_type":"stream"}],"execution_count":10}]}