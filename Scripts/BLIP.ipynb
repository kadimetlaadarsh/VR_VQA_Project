{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d81fe8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "# Load model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"/kaggle/input/merged-one/merged_vqa_dataset_output.csv\")  # Update with your actual filename\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    BASE_IMAGE_DIR = \"/kaggle/input/abo-dataset/images/small\"\n",
    "    full_image_path = os.path.join(BASE_IMAGE_DIR, str(row['path']))\n",
    "    image = Image.open(full_image_path).convert('RGB')\n",
    "    for q_col, a_col in zip(['q1', 'q2', 'q3'], ['a1', 'a2', 'a3']):\n",
    "        question = row[q_col]\n",
    "        gt_answer = row[a_col]\n",
    "        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs)\n",
    "        pred_answer = processor.decode(out[0], skip_special_tokens=True)\n",
    "        # Simple normalization for comparison\n",
    "        gt_answer_norm = str(gt_answer).strip().lower()\n",
    "        pred_answer_norm = str(pred_answer).strip().lower()\n",
    "        is_correct = (pred_answer_norm == gt_answer_norm)\n",
    "        all_results.append({\n",
    "            \"path\": row['path'],\n",
    "            \"question\": question,\n",
    "            \"gt_answer\": gt_answer,\n",
    "            \"pred_answer\": pred_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx+1}/{len(df)} images\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = results_df['is_correct'].mean()\n",
    "print(f\"Baseline Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Optionally, save results\n",
    "results_df.to_csv(\"vqa_baseline_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94369b67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "number_map = {\n",
    "    \"zero\": \"0\",\n",
    "    \"one\": \"1\",\n",
    "    \"two\": \"2\",\n",
    "    \"three\": \"3\",\n",
    "    \"four\": \"4\",\n",
    "    \"five\": \"5\",\n",
    "    \"six\": \"6\",\n",
    "    \"seven\": \"7\",\n",
    "    \"eight\": \"8\",\n",
    "    \"nine\": \"9\",\n",
    "    \"ten\": \"10\"\n",
    "}\n",
    "\n",
    "def normalize_answer(ans):\n",
    "    ans = str(ans).strip().lower()\n",
    "    # Convert number words to digits\n",
    "    if ans in number_map:\n",
    "        return number_map[ans]\n",
    "    # Also check if it's a digit string\n",
    "    if ans.isdigit():\n",
    "        return ans\n",
    "    # Normalize common yes/no answers\n",
    "    if ans in [\"yes\", \"no\"]:\n",
    "        return ans\n",
    "    # Optionally, add more normalization rules here\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47971030",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"/kaggle/input/merged-one/merged_vqa_dataset_output.csv\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "BASE_IMAGE_DIR = \"/kaggle/input/abo-dataset/images/small\"\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    full_image_path = os.path.join(BASE_IMAGE_DIR, str(row['path']))\n",
    "    image = Image.open(full_image_path).convert('RGB')\n",
    "    for q_col, a_col in zip(['q1', 'q2', 'q3'], ['a1', 'a2', 'a3']):\n",
    "        question = row[q_col]\n",
    "        gt_answer = row[a_col]\n",
    "        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs)\n",
    "        pred_answer = processor.decode(out[0], skip_special_tokens=True)\n",
    "        # Robust normalization for comparison\n",
    "        gt_answer_norm = normalize_answer(gt_answer)\n",
    "        pred_answer_norm = normalize_answer(pred_answer)\n",
    "        is_correct = (pred_answer_norm == gt_answer_norm)\n",
    "        all_results.append({\n",
    "            \"path\": row['path'],\n",
    "            \"question\": question,\n",
    "            \"gt_answer\": gt_answer,\n",
    "            \"pred_answer\": pred_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx+1}/{len(df)} images\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = results_df['is_correct'].mean()\n",
    "print(f\"Baseline Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Optionally, save results\n",
    "results_df.to_csv(\"vqa_baseline_results_normalized.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
